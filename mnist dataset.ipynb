{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras and tensorflow both are deep learning library\n",
    "#tensorflow act as backened of keras for low-level API computation\n",
    "#Kears is for high level computation(steps of keras: models,compilation,evaluation)\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tsf\n",
    "\n",
    "#keras have two types models:(1)Sequential (2)Functional \n",
    "from tensorflow.keras import models\n",
    "\n",
    "#in this dataset we apply convolutional neural network so we need conv layers\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#we import to_categorical because it convert our labels into categorical(0,1)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#import dataset because it is already present in keras library\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#For data visualization\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_images(train inputs)    #test_images(test inputs)\n",
    "#train_labels(train outputs)   #test_labels(test outputs)\n",
    "(train_images,train_labels),(test_images,test_labels)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1050ed30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images shape: (60000, 28, 28)\n",
      "train labels shape: (60000,)\n",
      "test images shape: (10000, 28, 28)\n",
      "test labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "#(60000,28,28) 60000=total train input images, 28,28(pixels of images)\n",
    "\n",
    "print(f\"train images shape: {train_images.shape}\")\n",
    "print(f\"train labels shape: {train_labels.shape}\")\n",
    "print(f\"test images shape: {test_images.shape}\")\n",
    "print(f\"test labels shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it shows that at position 7, (3) present\n",
    "train_labels[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#instantiate model\n",
    "model = models.Sequential()\n",
    "\n",
    "#adding 1st convolution layer\n",
    "#32= 32 filters/kernals  (3,3)=kernal/filter shape  \n",
    "model.add(layers.Conv2D(32, (3,3),activation=\"relu\",input_shape=(28,28,1)))\n",
    "#adding 1st maxpooling\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "#adding 2nd convolution layer\n",
    "model.add(layers.Conv2D(64,(3,3),activation=\"relu\",padding=\"same\"))\n",
    "#adding 2nd maxpooling\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "#adding 3rd convolution layer\n",
    "model.add(layers.Conv2D(32,(3,3),activation='relu'))\n",
    "\n",
    "#flatting data into 1D tensor beacuse dense layer accepts data in 1Dtensor\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "#adding Dense layer\n",
    "model.add(layers.Dense(64,activation='relu'))\n",
    "\n",
    "#output layer\n",
    "#softmax activation function used beacuse multiclass output\n",
    "model.add(layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 32)          18464     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 70,762\n",
      "Trainable params: 70,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the Train Image Array for compatible with CNN\n",
    "train_images = train_images.reshape((60000,28,28,1))\n",
    "\n",
    "# Normalizing the Train Images\n",
    "train_images = train_images.astype('float32')/255\n",
    "\n",
    "# Reshape the Test Image Array for compatible with CNN\n",
    "test_images = test_images.reshape((10000,28,28,1))\n",
    "\n",
    "#Normalizing the test images\n",
    "test_images = test_images.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 48s 806us/sample - loss: 0.1722 - acc: 0.9466\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 35s 577us/sample - loss: 0.0473 - acc: 0.9853\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 34s 573us/sample - loss: 0.0335 - acc: 0.9898\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 35s 576us/sample - loss: 0.0259 - acc: 0.9920\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 35s 591us/sample - loss: 0.0201 - acc: 0.9936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x605a7b8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compiling the CNN model\n",
    "model.compile(optimizer='rmsprop',loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "#fitting the model\n",
    "model.fit(train_images,train_labels,epochs=5,batch_size=64)\n",
    "\n",
    "#epochs(how many times the learning algorithm will work through enrire training dataset)\n",
    "#batch_size(number of training examples utilized in one iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 728us/sample - loss: 0.0292 - acc: 0.9920\n"
     ]
    }
   ],
   "source": [
    "#Evaluaate the CNN model\n",
    "test_loss,test_accuracy = model.evaluate(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy \n",
    "#perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
